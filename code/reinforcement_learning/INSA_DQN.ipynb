{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtsM5fhPgEuc"
      },
      "source": [
        "# Practical Session DQN\n",
        "\n",
        "In this practical session, you will implement the famous DQN algorithm and test it in various environments.\n",
        "\n",
        "Run the following two cells if you are using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3DsN79PDxHj",
        "outputId": "f81533a1-5f49-459b-8f02-f2bf866e3df7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E: Impossible d'ouvrir le fichier verrou /var/lib/dpkg/lock-frontend - open (13: Permission non accordée)\n",
            "E: Impossible d'obtenir le verrou de dpkg (/var/lib/dpkg/lock-frontend). Avez-vous les droits du superutilisateur ?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym[box2d]==0.17.*\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 13.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Collecting PyOpenGL==3.1.*\n",
            "  Downloading PyOpenGL-3.1.6-py3-none-any.whl (2.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 39.6 MB/s eta 0:00:00\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 538.4/538.4 kB 32.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: scipy in /home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (1.8.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages (from gym[box2d]==0.17.*) (1.22.4)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 27.9 MB/s eta 0:00:00\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 374.5/374.5 kB 43.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 829.2/829.2 kB 33.6 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: PyOpenGL-accelerate, box2d-py, gym, future\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): started\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): finished with status 'done'\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp39-cp39-linux_x86_64.whl size=501834 sha256=ebb9f5a740795c16dedb268c4fb7fc2913230afdde61e68f2129c7d506f88c44\n",
            "  Stored in directory: /home/n7student/.cache/pip/wheels/32/bf/f2/fc5d1e29923dab6c757eca3fe7265b9a5b0c0550a08bc1307f\n",
            "  Building wheel for box2d-py (setup.py): started\n",
            "  Building wheel for box2d-py (setup.py): finished with status 'done'\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp39-cp39-linux_x86_64.whl size=458918 sha256=52b7a402f9ae0a35488960229b5ceb1350456a23031d9df507599fc6777b8de2\n",
            "  Stored in directory: /home/n7student/.cache/pip/wheels/a0/6a/54/2383a12859109612796c6b235ff6704a82b99caa4308ca0b43\n",
            "  Building wheel for gym (setup.py): started\n",
            "  Building wheel for gym (setup.py): finished with status 'done'\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654622 sha256=b05ae83660092d47271f814484123d852e404e6b19f4a01e3eea56bd83e4b15e\n",
            "  Stored in directory: /home/n7student/.cache/pip/wheels/5d/06/a4/57a926b2e87a5e1f21551577750549206dde639e19a5ac72d5\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=fb10638f1a640e5d1cd56e1b1a6cf5f8e5ba564957743aa65250526a79f8ef5b\n",
            "  Stored in directory: /home/n7student/.cache/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
            "Successfully built PyOpenGL-accelerate box2d-py gym future\n",
            "Installing collected packages: PyOpenGL-accelerate, PyOpenGL, EasyProcess, box2d-py, pyvirtualdisplay, future, cloudpickle, pyglet, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.1.0\n",
            "    Uninstalling cloudpickle-2.1.0:\n",
            "      Successfully uninstalled cloudpickle-2.1.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.24.1\n",
            "    Uninstalling gym-0.24.1:\n",
            "      Successfully uninstalled gym-0.24.1\n",
            "Successfully installed EasyProcess-1.1 PyOpenGL-3.1.6 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 cloudpickle-1.6.0 future-0.18.2 gym-0.17.3 pyglet-1.5.0 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gew6ipsBD1Yo"
      },
      "outputs": [
        {
          "ename": "EasyProcessError",
          "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py:176\u001b[0m, in \u001b[0;36mEasyProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=174'>175</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=175'>176</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopen \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mPopen(\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=176'>177</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcmd,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=177'>178</a>\u001b[0m         stdout\u001b[39m=\u001b[39;49mstdout,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=178'>179</a>\u001b[0m         stderr\u001b[39m=\u001b[39;49mstderr,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=179'>180</a>\u001b[0m         cwd\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcwd,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=180'>181</a>\u001b[0m         env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=181'>182</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=182'>183</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m oserror:\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=947'>948</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=948'>949</a>\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=950'>951</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=951'>952</a>\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=952'>953</a>\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=953'>954</a>\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=954'>955</a>\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=955'>956</a>\u001b[0m                         errread, errwrite,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=956'>957</a>\u001b[0m                         restore_signals,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=957'>958</a>\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=958'>959</a>\u001b[0m                         start_new_session)\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=959'>960</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=960'>961</a>\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/subprocess.py:1821\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=1819'>1820</a>\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=1820'>1821</a>\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/subprocess.py?line=1821'>1822</a>\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mEasyProcessError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/home/n7student/Téléchargements/INSA_DQN.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/n7student/T%C3%A9l%C3%A9chargements/INSA_DQN.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyvirtualdisplay\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/n7student/T%C3%A9l%C3%A9chargements/INSA_DQN.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/n7student/T%C3%A9l%C3%A9chargements/INSA_DQN.ipynb#ch0000002?line=3'>4</a>\u001b[0m _display \u001b[39m=\u001b[39m pyvirtualdisplay\u001b[39m.\u001b[39;49mDisplay(visible\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# use False with Xvfb\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/n7student/T%C3%A9l%C3%A9chargements/INSA_DQN.ipynb#ch0000002?line=4'>5</a>\u001b[0m                                     size\u001b[39m=\u001b[39;49m(\u001b[39m1400\u001b[39;49m, \u001b[39m900\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/n7student/T%C3%A9l%C3%A9chargements/INSA_DQN.ipynb#ch0000002?line=5'>6</a>\u001b[0m _ \u001b[39m=\u001b[39m _display\u001b[39m.\u001b[39mstart()\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py:34\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, check_startup, randomizer, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=30'>31</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=31'>32</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mxvfb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay_class(\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=34'>35</a>\u001b[0m     size\u001b[39m=\u001b[39msize,\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=35'>36</a>\u001b[0m     color_depth\u001b[39m=\u001b[39mcolor_depth,\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=36'>37</a>\u001b[0m     bgcolor\u001b[39m=\u001b[39mbgcolor,\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=37'>38</a>\u001b[0m     randomizer\u001b[39m=\u001b[39mrandomizer,\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=38'>39</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=39'>40</a>\u001b[0m AbstractDisplay\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, use_xauth\u001b[39m=\u001b[39muse_xauth, check_startup\u001b[39m=\u001b[39mcheck_startup, randomizer\u001b[39m=\u001b[39mrandomizer)\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py:53\u001b[0m, in \u001b[0;36mDisplay.display_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=49'>50</a>\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m XephyrDisplay\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=51'>52</a>\u001b[0m \u001b[39m# TODO: check only once\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=52'>53</a>\u001b[0m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_installed()\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/display.py?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py:40\u001b[0m, in \u001b[0;36mXvfbDisplay.check_installed\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py?line=37'>38</a>\u001b[0m p\u001b[39m.\u001b[39menable_stdout_log \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py?line=38'>39</a>\u001b[0m p\u001b[39m.\u001b[39menable_stderr_log \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py?line=39'>40</a>\u001b[0m p\u001b[39m.\u001b[39;49mcall()\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py:147\u001b[0m, in \u001b[0;36mEasyProcess.call\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=135'>136</a>\u001b[0m \u001b[39m\"\"\"Run command with arguments. Wait for command to complete.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=136'>137</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=137'>138</a>\u001b[0m \u001b[39msame as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=143'>144</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=144'>145</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=145'>146</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart()\u001b[39m.\u001b[39mwait(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=147'>148</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=148'>149</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_alive():\n",
            "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py:186\u001b[0m, in \u001b[0;36mEasyProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=183'>184</a>\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mOSError exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, oserror)\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=184'>185</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moserror \u001b[39m=\u001b[39m oserror\n\u001b[0;32m--> <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=185'>186</a>\u001b[0m     \u001b[39mraise\u001b[39;00m EasyProcessError(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstart error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=186'>187</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_started \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/n7student/anaconda3/envs/gym/lib/python3.9/site-packages/easyprocess/__init__.py?line=187'>188</a>\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mprocess was started (pid=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid)\n",
            "\u001b[0;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
          ]
        }
      ],
      "source": [
        "import pyvirtualdisplay\n",
        "import Box2D\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWbiTnogjch"
      },
      "source": [
        "# Lunar Lander\n",
        "[Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) is a reinforcement learning toy environment provided with the Gym Library.  \n",
        "In this environment, you control the reactors of a Lander that tries to land inside a landing area as smoothly as possible. \n",
        "The official environment description is  \n",
        "*Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.*\n",
        "\n",
        "The environment will provide observations to the agent in the form of an 8-dimensional vector composed of:\n",
        "* Position X\n",
        "* Position Y\n",
        "* Velocity X\n",
        "* Velocity Y\n",
        "* Angle\n",
        "* Angular Velocity\n",
        "* Is left leg touching the ground: 0 OR 1\n",
        "* Is right leg touching the ground: 0 OR 1  \n",
        "\n",
        "The agent can choose between four discrete actions:\n",
        "* 0 = Do Nothing\n",
        "* 1 = Fire Left Engine\n",
        "* 2 = Fire Main Engine\n",
        "* 3 = Fire Right Engine  \n",
        "\n",
        "At every transition, the agent will be rewarded by the environment according to the following reward function:\n",
        "\n",
        "* [100, 140] points for Moving to the landing pad and zero speed\n",
        "* Negative reward for moving away from the landing pad\n",
        "* If the lander crashes or comes to rest, it gets -100 or +100\n",
        "* Each leg with ground contact gets +10\n",
        "* Firing the main engine is -0.3 per frame\n",
        "* Firing the side engine is -0.03 per frame\n",
        "* 200 points for solving the environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yaa8cSiXy-w"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from IPython.display import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx7AYLSnX7PE"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfLhHuhJYA3W"
      },
      "outputs": [],
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQpj2vkyivwL"
      },
      "source": [
        "Here is a little code snippet to visualise a random agent playing Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbp4SnIDuEj"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_random.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_random.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcGoe9ZjzF4"
      },
      "source": [
        "We will now implement and train a DQN agent.\n",
        "Fill the following code to create a neural network class that our agent will use.\n",
        "The network should be small enough not to slow down the training process.  \n",
        "You can use a two hidden layers neural network with 512 and 256 hidden neurons using a ReLU activation function and a final layer with as many neurons as the environment action space using a linear activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lnVAsTZgOnF"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.fc2 = nn.Linear(256, )\n",
        "        self.fc3 = nn.Linear(256, action_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "    \n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA_sdNSqj224"
      },
      "source": [
        "The following class implements a replay buffer that will act as our agent memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQS7b95VgPlN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        \"\"\"\n",
        "        Replay memory allow agent to record experiences and learn from them\n",
        "        \n",
        "        Parametes\n",
        "        ---------\n",
        "        buffer_size (int): maximum size of internal memory\n",
        "        batch_size (int): sample size from experience\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add experience\"\"\"\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "                \n",
        "    def sample(self):\n",
        "        \"\"\" \n",
        "        Sample randomly and return (state, action, reward, next_state, done) tuple as torch tensors \n",
        "        \"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        # Convert to torch tensors\n",
        "        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(device)        \n",
        "        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(device)        \n",
        "        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(device)  \n",
        "        # Convert done from boolean to int\n",
        "        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(device)        \n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-YTCBEQDuEm"
      },
      "source": [
        "# DQN\n",
        "The fllowing class implements a DQN agent.  \n",
        "Its training procedure is located in the `train` method.  \n",
        "Fill the missing parts of the `act` and `learn` methods according to the DQN algorithm:\n",
        "![](https://github.com/wikistat/AI-Frameworks/blob/website/code/reinforcement_learning/images/dqn.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwxcpmGZgTEs"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3               # Q Network learning rate\n",
        "EPS_DECAY = 0.999    # Epsilon decay rate\n",
        "EPS_MIN = 0.01  #min Epsilon\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def print_running_mean(training_rewards):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(15,3))\n",
        "    plt.plot(pd.Series(training_rewards).rolling(10).mean())\n",
        "    plt.title(\"Rewards running mean on last 100 episodes\")\n",
        "    plt.show()\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4):\n",
        "        self.env =  env\n",
        "        self.gamma = gamma\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.update_rate = 4\n",
        "        self.batch_size = batch_size\n",
        "        self.q_network = QNetwork(self.state_size, self.action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001, weight_decay=1e-6)\n",
        "        # Initiliase memory \n",
        "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "        \n",
        "                \n",
        "    def train(self, max_steps):\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        current_reward = 0\n",
        "        episode_rewards = []\n",
        "        for _ in range(max_steps):\n",
        "            action = self.act(state, train=True)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            current_reward += reward\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "            self.timestep += 1\n",
        "            if (self.timestep % self.update_rate == 0) & (len(self.memory) > self.batch_size):\n",
        "                self.learn()\n",
        "                \n",
        "            self.epsilon = max(self.epsilon * EPS_DECAY, EPS_MIN)\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                episode_rewards.append(current_reward)\n",
        "                current_reward = 0\n",
        "                \n",
        "            else:\n",
        "                state = next_state\n",
        "            if self.timestep % 1000 == 0:\n",
        "                print_running_mean(episode_rewards)\n",
        "                \n",
        "                \n",
        "    def learn(self):\n",
        "        experiences_batch = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = self.q_network(next_states).detach() #Do you know why is this detach important?\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        \n",
        "        loss = nn.MSELoss()(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        \n",
        "    def act(self, state, train=False):\n",
        "\n",
        "        if (random.uniform(0, 1) < self.epsilon) & train:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4TBIZ8jlcIV"
      },
      "source": [
        "Instanciate a DQN agent and train it on lunar lander for at least 100000 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lMIifWGDuEp"
      },
      "outputs": [],
      "source": [
        "dqn_agent = DQNAgent(env)\n",
        "dqn_agent.train(max_steps=1000000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fna62-OWoLsg"
      },
      "source": [
        "Complete the following code to visualize your agent playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH2su8-2DuEp"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_dqn.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_dqn.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUOs7jll5Xs"
      },
      "source": [
        "An important impovement of DQN is the use of a target network.  \n",
        "Modify your DQN agent to use a secondary target network.  \n",
        "![](https://github.com/DavidBert/N7-techno-IA/raw/master/code/reinforcement_learning/images/dqn_target.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3b-UoJCDuEr"
      },
      "outputs": [],
      "source": [
        "class DQNTargetAgent(DQNAgent):\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4, target_update_rate=1000):\n",
        "        super().__init__(env, gamma, memory_size, batch_size, update_rate)\n",
        "        self.target_update_rate = target_update_rate\n",
        "        self.target_network = QNetwork(self.state_size, self.action_size).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict()) #initialise the target network weights with the policy network's weights\n",
        "        \n",
        "    def learn(self):\n",
        "        experiences_batch = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = self.q_network(next_states).detach() \n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        \n",
        "        loss = nn.MSELoss()(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if self.timestep % self.target_update_rate == 0: #update the target network weights with the policy network's weights\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgxf30dhvs-R"
      },
      "source": [
        "Train a dqn agent with target network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieI4TJEGghuF"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = DQNTargetAgent(env)\n",
        "dqn_target_agent.train(max_steps=1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IHzWpU7of98"
      },
      "source": [
        "Visualize your agent playing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzJQmz0DuEr"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_target_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_target.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_target.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBK075MfokmX"
      },
      "source": [
        "The following code compares all the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC84Y4FZDuEs"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVoMC79Xi81N"
      },
      "source": [
        "# Cartpole\n",
        "\n",
        "CartPole is another [toy control environment avalible in OpeAI Gym](https://gym.openai.com/envs/CartPole-v0/). \n",
        "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.* \n",
        "\n",
        "It is also known as the inverted pendulum problem.  \n",
        "In this environment the objective is to balance a pole as long as possible. It is assumed that at the tip of the pole, there is an object which makes it unstable and very likely to fall over. The agent controls the cart and must move left and right so that the pole can stand as long as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePTgwYFDuEs"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVI6Rhv1DuEt"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_random.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_random.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMbTPmrp2LV"
      },
      "source": [
        "Train a dqn agent to solve cartpole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shLtAxhki8Da"
      },
      "outputs": [],
      "source": [
        "dqn_agent = DQNAgent(env)\n",
        "dqn_agent.train(max_steps=1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQSCBQpDuEu"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_dqn.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_dqn.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00SrU1GwqESW"
      },
      "source": [
        "Train a dqn agent to solve cartpole using target network and compare its performances with the simple DQN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "496w98bDDuEu"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = DQNTargetAgent(env)\n",
        "#you may update the target network every 500 steps\n",
        "dqn_target_agent.train(max_steps=1000000, target_update_rate=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Z65WQtDuEv"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_target_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_target.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_target.gif','rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgzubhZHDuEv"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wlteZoGdij"
      },
      "source": [
        "# Stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQSNcugqO0k"
      },
      "source": [
        "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It contains efficient implementations of the most famous RL algorithm and is certainly one of the most simple way to prototype an RL agent.  \n",
        "A full description of the librairy is available [here](https://stable-baselines3.readthedocs.io/en/master/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrQonYsIJc-"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_1m6EYrBG5"
      },
      "source": [
        "Help yourself with the official documentation and train a dqn agent using Stable baselines3 on Lunar lander."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mvUTfx3Gg5s"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Instantiate the agent\n",
        "model = DQN('MlpPolicy', env, learning_rate=1e-3)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACI1HpUcweXb"
      },
      "source": [
        "Compare its performance with our implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6D0qRYrMF3i"
      },
      "outputs": [],
      "source": [
        "scores = []\n",
        "for i in tqdm(range(max_episodes)):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scores.append(score)\n",
        "for score in scores:\n",
        "    df = df.append({'rewards': score, 'agent': 'stable_baseline'}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwCiho6MmzR"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./stable_baselines.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('stable_baselines.gif','rb').read())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "INSA_DQN.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3b5367d8d8491bdefee5e6f82db73b5f8699223173e25001ec984e8a6adfa422"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('gym')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
