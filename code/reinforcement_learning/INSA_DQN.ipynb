{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtsM5fhPgEuc"
      },
      "source": [
        "# Practical Session DQN\n",
        "\n",
        "In this practical session, you will implement the famous DQN algorithm and test it in various environments.\n",
        "\n",
        "Run the following two cells if you are using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3DsN79PDxHj",
        "outputId": "751c50cd-168a-46e5-d255-be5a1e076d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.6)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (1.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.21.6)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gew6ipsBD1Yo"
      },
      "outputs": [],
      "source": [
        "import pyvirtualdisplay\n",
        "import Box2D\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWbiTnogjch"
      },
      "source": [
        "# Lunar Lander\n",
        "[Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) is a reinforcement learning toy environment provided with the Gym Library.  \n",
        "In this environment, you control the reactors of a Lander that tries to land inside a landing area as smoothly as possible. \n",
        "The official environment description is  \n",
        "*Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.*\n",
        "\n",
        "The environment will provide observations to the agent in the form of an 8-dimensional vector composed of:\n",
        "* Position X\n",
        "* Position Y\n",
        "* Velocity X\n",
        "* Velocity Y\n",
        "* Angle\n",
        "* Angular Velocity\n",
        "* Is left leg touching the ground: 0 OR 1\n",
        "* Is right leg touching the ground: 0 OR 1  \n",
        "\n",
        "The agent can choose between four discrete actions:\n",
        "* 0 = Do Nothing\n",
        "* 1 = Fire Left Engine\n",
        "* 2 = Fire Main Engine\n",
        "* 3 = Fire Right Engine  \n",
        "\n",
        "At every transition, the agent will be rewarded by the environment according to the following reward function:\n",
        "\n",
        "* [100, 140] points for Moving to the landing pad and zero speed\n",
        "* Negative reward for moving away from the landing pad\n",
        "* If the lander crashes or comes to rest, it gets -100 or +100\n",
        "* Each leg with ground contact gets +10\n",
        "* Firing the main engine is -0.3 per frame\n",
        "* Firing the side engine is -0.03 per frame\n",
        "* 200 points for solving the environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0Yaa8cSiXy-w"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from IPython.display import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yx7AYLSnX7PE"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GfLhHuhJYA3W",
        "outputId": "6dd81d66-bc05-4334-876c-3b34c7fb20f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n",
            "Box(-inf, inf, (8,), float32)\n"
          ]
        }
      ],
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQpj2vkyivwL"
      },
      "source": [
        "Here is a little code snippet to visualise a random agent playing Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bJbp4SnIDuEj",
        "outputId": "8c13c3ea-5b81-4212-cb19-6decdef701f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5db2ef675dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIEWPORT_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVIEWPORT_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_random.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_random.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcGoe9ZjzF4"
      },
      "source": [
        "We will now implement and train a DQN agent.\n",
        "Fill the following code to create a neural network class that our agent will use.\n",
        "The network should be small enough not to slow down the training process.  \n",
        "You can use a two hidden layers neural network with 512 and 256 hidden neurons using a ReLU activation function and a final layer with as many neurons as the environment action space using a linear activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lnVAsTZgOnF"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, action_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "    \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA_sdNSqj224"
      },
      "source": [
        "The following class implements a replay buffer that will act as our agent memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQS7b95VgPlN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        \"\"\"\n",
        "        Replay memory allow agent to record experiences and learn from them\n",
        "        \n",
        "        Parametes\n",
        "        ---------\n",
        "        buffer_size (int): maximum size of internal memory\n",
        "        batch_size (int): sample size from experience\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add experience\"\"\"\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "                \n",
        "    def sample(self):\n",
        "        \"\"\" \n",
        "        Sample randomly and return (state, action, reward, next_state, done) tuple as torch tensors \n",
        "        \"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        # Convert to torch tensors\n",
        "        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(device)        \n",
        "        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(device)        \n",
        "        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(device)  \n",
        "        # Convert done from boolean to int\n",
        "        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(device)        \n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-YTCBEQDuEm"
      },
      "source": [
        "# DQN\n",
        "The fllowing class implements a DQN agent.  \n",
        "Its training procedure is located in the `train` method.  \n",
        "Fill the missing parts of the `act` and `learn` methods according to the DQN algorithm:\n",
        "![](https://github.com/wikistat/AI-Frameworks/blob/website/code/reinforcement_learning/images/dqn.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwxcpmGZgTEs"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3               # Q Network learning rate\n",
        "EPS_DECAY = 0.999    # Epsilon decay rate\n",
        "EPS_MIN = 0.01  #min Epsilon\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def print_running_mean(training_rewards):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(15,3))\n",
        "    plt.plot(pd.Series(training_rewards).rolling(10).mean())\n",
        "    plt.title(\"Rewards running mean on last 100 episodes\")\n",
        "    plt.show()\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4):\n",
        "        self.env =  env\n",
        "        self.gamma = gamma\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.update_rate = 4\n",
        "        self.batch_size = batch_size\n",
        "        self.q_network = QNetwork(self.state_size, self.action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001, weight_decay=1e-6)\n",
        "        # Initiliase memory \n",
        "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "        \n",
        "                \n",
        "    def train(self, max_steps):\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        current_reward = 0\n",
        "        episode_rewards = []\n",
        "        for _ in range(max_steps):\n",
        "            action = self.act(state, train=True)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            current_reward += reward\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "            self.timestep += 1\n",
        "            if (self.timestep % self.update_rate == 0) & (len(self.memory) > self.batch_size):\n",
        "                self.learn()\n",
        "                \n",
        "            self.epsilon = max(self.epsilon * EPS_DECAY, EPS_MIN)\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                episode_rewards.append(current_reward)\n",
        "                current_reward = 0\n",
        "                \n",
        "            else:\n",
        "                state = next_state\n",
        "            if self.timestep % 1000 == 0:\n",
        "                print_running_mean(episode_rewards)\n",
        "                \n",
        "                \n",
        "    def learn(self):\n",
        "        experiences_batch = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = self.q_network(next_states).detach() #Do you know why is this detach important?\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        \n",
        "        loss = nn.MSELoss()(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        \n",
        "    def act(self, state, train=False):\n",
        "\n",
        "        if (random.uniform(0, 1) < self.epsilon) & train:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4TBIZ8jlcIV"
      },
      "source": [
        "Instanciate a DQN agent and train it on lunar lander for at least 100000 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lMIifWGDuEp"
      },
      "outputs": [],
      "source": [
        "dqn_agent = DQNAgent(env)\n",
        "dqn_agent.train(max_steps=100000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fna62-OWoLsg"
      },
      "source": [
        "Complete the following code to visualize your agent playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH2su8-2DuEp"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_dqn.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_dqn.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUOs7jll5Xs"
      },
      "source": [
        "An important impovement of DQN is the use of a target network.  \n",
        "Modify your DQN agent to use a secondary target network.  \n",
        "![](https://github.com/DavidBert/N7-techno-IA/raw/master/code/reinforcement_learning/images/dqn_target.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3b-UoJCDuEr"
      },
      "outputs": [],
      "source": [
        "class DQNTargetAgent(DQNAgent):\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4, target_update_rate=1000):\n",
        "        super().__init__(env, gamma, memory_size, batch_size, update_rate)\n",
        "        self.target_update_rate = target_update_rate\n",
        "        self.target_network = QNetwork(self.state_size, self.action_size).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict()) #initialise the target network weights with the policy network's weights\n",
        "        \n",
        "    def learn(self):\n",
        "        experiences_batch = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = self.q_network(next_states).detach() \n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        \n",
        "        loss = nn.MSELoss()(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if self.timestep % self.target_update_rate == 0: #update the target network weights with the policy network's weights\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgxf30dhvs-R"
      },
      "source": [
        "Train a dqn agent with target network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieI4TJEGghuF"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = DQNTargetAgent(env)\n",
        "dqn_target_agent.train(max_steps=1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IHzWpU7of98"
      },
      "source": [
        "Visualize your agent playing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzJQmz0DuEr"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_target_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./lunar_target.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('lunar_target.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBK075MfokmX"
      },
      "source": [
        "The following code compares all the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC84Y4FZDuEs"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVoMC79Xi81N"
      },
      "source": [
        "# Cartpole\n",
        "\n",
        "CartPole is another [toy control environment avalible in OpeAI Gym](https://gym.openai.com/envs/CartPole-v0/). \n",
        "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.* \n",
        "\n",
        "It is also known as the inverted pendulum problem.  \n",
        "In this environment the objective is to balance a pole as long as possible. It is assumed that at the tip of the pole, there is an object which makes it unstable and very likely to fall over. The agent controls the cart and must move left and right so that the pole can stand as long as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePTgwYFDuEs"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVI6Rhv1DuEt"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_random.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_random.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMbTPmrp2LV"
      },
      "source": [
        "Train a dqn agent to solve cartpole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shLtAxhki8Da"
      },
      "outputs": [],
      "source": [
        "dqn_agent = DQNAgent(env)\n",
        "dqn_agent.train(max_steps=1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQSCBQpDuEu"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_dqn.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_dqn.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00SrU1GwqESW"
      },
      "source": [
        "Train a dqn agent to solve cartpole using target network and compare its performances with the simple DQN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "496w98bDDuEu"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = DQNTargetAgent(env)\n",
        "#you may update the target network every 500 steps\n",
        "dqn_target_agent.train(max_steps=1000000, target_update_rate=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Z65WQtDuEv"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_target_agent.act(obs)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./cartpole_target.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('cartpole_target.gif','rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgzubhZHDuEv"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wlteZoGdij"
      },
      "source": [
        "# Stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQSNcugqO0k"
      },
      "source": [
        "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It contains efficient implementations of the most famous RL algorithm and is certainly one of the most simple way to prototype an RL agent.  \n",
        "A full description of the librairy is available [here](https://stable-baselines3.readthedocs.io/en/master/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrQonYsIJc-"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_1m6EYrBG5"
      },
      "source": [
        "Help yourself with the official documentation and train a dqn agent using Stable baselines3 on Lunar lander."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mvUTfx3Gg5s"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Instantiate the agent\n",
        "model = DQN('MlpPolicy', env, learning_rate=1e-3)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluate the agent\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACI1HpUcweXb"
      },
      "source": [
        "Compare its performance with our implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6D0qRYrMF3i"
      },
      "outputs": [],
      "source": [
        "scores = []\n",
        "for i in tqdm(range(max_episodes)):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scores.append(score)\n",
        "for score in scores:\n",
        "    df = df.append({'rewards': score, 'agent': 'stable_baseline'}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwCiho6MmzR"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "env.close()\n",
        "imageio.mimwrite('./stable_baselines.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('stable_baselines.gif','rb').read())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "INSA_DQN.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3b5367d8d8491bdefee5e6f82db73b5f8699223173e25001ec984e8a6adfa422"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('gym')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}